{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ddd236c-c4f4-4449-93fa-43b0db69b185",
   "metadata": {},
   "source": [
    "# **Comprehensive NLP Analysis of Global News Headlines (2019-Present)**\n",
    "\n",
    "This document presents a **thorough Natural Language Processing (NLP) analysis** of global news headlines spanning from **2019 to the present**. The analysis demonstrates proficiency in fundamental NLP techniques, preprocessing steps, and insightful visualizations that extract meaningful patterns from textual data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [**Introduction**](#introduction)\n",
    "2. [**Data Exploration**](#data-exploration)\n",
    "3. [**Data Preprocessing**](#data-preprocessing)\n",
    "   - [Text Normalization](#text-normalization)\n",
    "   - [Tokenization](#tokenization)\n",
    "   - [Stop Words Removal](#stop-words-removal)\n",
    "   - [Token Unification and Renormalizing Entities](#token-unification)\n",
    "   - [Part-of-Speech Tagging](#part-of-speech-tagging)\n",
    "   - [Lemmatization](#lemmatization)\n",
    "   - [Named Entity Recognition](#named-entity-recognition)\n",
    "4. [**Feature Extraction**](#feature-extraction)\n",
    "   - [Bag of Words](#bag-of-words)\n",
    "   - [TF-IDF Vectorization](#tf-idf-vectorization)\n",
    "   - [Word Embeddings](#word-embeddings)\n",
    "5. [**Sentiment Analysis**](#sentiment-analysis)\n",
    "   - [Monthly Sentiment Trends](#monthly-sentiment-trends)\n",
    "   - [Year-over-Year Sentiment Comparison](#year-over-year-sentiment-comparison)\n",
    "6. [**Topic Modeling**](#topic-modeling)\n",
    "   - [Latent Dirichlet Allocation](#latent-dirichlet-allocation)\n",
    "   - [Topic Evolution Over Time](#topic-evolution-over-time)\n",
    "7. [**Entity Analysis**](#entity-analysis)\n",
    "   - [Most Mentioned Entities](#most-mentioned-entities)\n",
    "   - [Entity Co-occurrence Networks](#entity-co-occurrence-networks)\n",
    "8. [**Time Series Analysis**](#time-series-analysis)\n",
    "   - [Headline Complexity Over Time](#headline-complexity-over-time)\n",
    "   - [Topic Seasonality](#topic-seasonality)\n",
    "9. [**Conclusion**](#conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This analysis explores a rich dataset of **news headlines** from **2019 to 2023**, covering **25 of the world's most influential news headlines**. The dataset is structured with **dates** in the first column followed by 25 headlines from each source. By applying various **NLP techniques**, we aim to uncover patterns, trends, and insights that reveal how global news discourse has evolved over this significant period, from sentimental analysis of each day and year to reoccuring patterns of entities and how their sentiment on the media has changed over the years, or as i like to call it, the sentimental derivative. \n",
    "\n",
    "---\n",
    "\n",
    "## **Data Exploration**\n",
    "\n",
    "Let's begin by loading the dataset and exploring its basic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662a84a6-a593-44f9-9996-2dc382e502f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load the dataset\n",
    "news_df = pd.read_csv('datasets/WorldNewsData.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {news_df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# Check the first few rows\n",
    "\n",
    "\n",
    "# Convert Date column to datetime (note capital \"D\")\n",
    "# Since the date format is \"May 01, 2018\", we need to parse it correctly\n",
    "news_df['Date'] = pd.to_datetime(news_df['Date'], format=\"%b %d, %Y\")\n",
    "\n",
    "# Display the date range\n",
    "print(f\"\\nTime period: {news_df['Date'].min()} to {news_df['Date'].max()}\")\n",
    "\n",
    "# Create a year-month column for temporal analysis\n",
    "news_df['year_month'] = news_df['Date'].dt.to_period('M')\n",
    "\n",
    "# Create a column for the year\n",
    "news_df['year'] = news_df['Date'].dt.year\n",
    "\n",
    "# Create a column for the month\n",
    "news_df['month'] = news_df['Date'].dt.month\n",
    "\n",
    "# Sample a few rows\n",
    "print(\"\\nSample data:\")\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99f933-f631-4efe-ae75-a3f01f72429a",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**\n",
    "\n",
    " There are some duplicate months in the original data, (like October 2020) Lets start by removing the duplicates and clearing the dataset. Next, let's create a function to combine all headlines for each day into a single text corpus, and a list that contains the headlines separately, so we can analyze both on per day basis and per headline. This distinction will be useful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bddda-5df7-4a79-a1d0-46eb326fd204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring 'Date' is in datetime format\n",
    "news_df['Date'] = pd.to_datetime(news_df['Date'])\n",
    "\n",
    "# drop duplicates\n",
    "news_df = news_df.drop_duplicates(subset='Date', keep='first')\n",
    "\n",
    "def combine_headlines(row):\n",
    "    # combine all headlines in a row into a single string for easy day based tokenization later\n",
    "    headlines = []\n",
    "    # start from column 'Top1' through 'Top25'\n",
    "    for col in news_df.columns[1:26]:  # Skip the Date column, include only Top1-Top25\n",
    "        if pd.notna(row[col]):\n",
    "            headlines.append(str(row[col]))\n",
    "    return ' '.join(headlines)\n",
    "\n",
    "# Apply the function to create a new column with combined headlines\n",
    "news_df['combined_headlines'] = news_df.apply(combine_headlines, axis=1)\n",
    "\n",
    "headlines_per_month = news_df.groupby(news_df['Date'].dt.to_period('M')).size() * 25\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "headlines_per_month.plot(kind='bar')\n",
    "plt.title('Total Number of Headlines per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Headline Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a new column that stores the list of non-null individual headlines (Top1–Top25)\n",
    "def get_separate_headlines(row):\n",
    "    \"\"\"Return a list of non-empty individual headlines\"\"\"\n",
    "    return [str(row[col]) for col in news_df.columns[1:26] if pd.notna(row[col])]\n",
    "\n",
    "news_df['separate_headlines'] = news_df.apply(get_separate_headlines, axis=1)\n",
    "\n",
    "# Count number of headlines (not assuming all 25 filled)\n",
    "news_df['headline_count'] = news_df['separate_headlines'].apply(len)\n",
    "\n",
    "# Group by month and sum all actual headlines\n",
    "separate_headlines_per_month = news_df.groupby(news_df['Date'].dt.to_period('M'))['headline_count'].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f562d93-9aaa-4e87-904f-6a8d605886ed",
   "metadata": {},
   "source": [
    "# **Text Normalization**\n",
    "\n",
    "Now our data looks clear and no unusual spikes showing duplicate data. Before diving into the deeper depths of data processing, we need to normalize our dataset. Normalization is like organizing a dataset, we need to remove symbols, lowercase everything, remove numbers, non-UNICODE letters, and some other things based on what we need, we might need to remove words like \"in, and, out, of\" (which are called stopwords) because they might skew the data by introducing unnecessary bias. We actually took our first step already by removing duplicates! We will do a general normalization here and might use more in depth techniques like lemmatization or stemming based on our needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d5cb6a",
   "metadata": {},
   "source": [
    "Next, lets start by defining a new function normalize_text() to do every normalization step we desire to the dataset and store it in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ab2cf-cb84-4432-8ac3-a91c9f4b4b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy contractions\n",
    "\n",
    "# python -m spacy download en_core_web_sm\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "import contractions\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy model (blank English model to avoid over-processing)\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def safe_expand_contractions(text):\n",
    "    \"\"\"Safely expand contractions, handling non-English text\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return text\n",
    "    \n",
    "    # Skip processing if text contains non-ASCII characters that might cause issues\n",
    "    if any(ord(c) > 127 for c in text):\n",
    "        return text  # Return original text without expansion\n",
    "    \n",
    "    # Process contractions for English text\n",
    "    try:\n",
    "        return contractions.fix(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Contraction expansion error: {e} in text: {text[:50]}...\")\n",
    "        return text  # Return original text on error\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "        \n",
    "    # Step 1: Pre-replace acronyms with more comprehensive dict\n",
    "    acronyms = {\"U.N.\": \"UN\", \"U.S.\": \"US\", \"E.U.\": \"EU\", \"u.s.\": \"US\", \n",
    "                \"U.K.\": \"UK\", \"N.Y.\": \"NY\", \"L.A.\": \"LA\"}\n",
    "    for k, v in acronyms.items():\n",
    "        text = text.replace(k, v)\n",
    "    \n",
    "    # Step 2: Expand contractions with safe handling\n",
    "    text = safe_expand_contractions(text)\n",
    "    \n",
    "    # Step 3: Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 4: Remove possessive 's or 's (improved patterns)\n",
    "    text = re.sub(r\"\\b(\\w+)['']\\s*s\\b\", r\"\\1\", text)  # Handles standard possessives\n",
    "    text = re.sub(r\"\\b(\\w+s)['']\\b\", r\"\\1\", text)     # Handles plural possessives\n",
    "    \n",
    "    # Step 5: Remove remaining apostrophes (shouldn't be needed, for edge cases)\n",
    "    text = re.sub(r\"(\\w+)'(\\w+)\", r\"\\1\\2\", text)\n",
    "    text = re.sub(r\"(\\w+)'\", r\"\\1\", text)\n",
    "    \n",
    "    # Step 6: Remove punctuation and smart quotes/dashes\n",
    "    punct_chars = string.punctuation + \"–—''\"\"•\"\n",
    "    text = re.sub(f\"[{re.escape(punct_chars)}]\", \"\", text)\n",
    "    \n",
    "    # Step 7: HTML unescape and entity removal (combined)\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'&[a-zA-Z#0-9]+;', '', text)  # Remove any remaining entities\n",
    "    \n",
    "    # Step 8: Remove any non-English characters and emojis\n",
    "    # Note: This will remove numbers as per original code\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Step 9: Remove any remaining single s (from cases like \"60's\")\n",
    "    text = re.sub(r'\\b[sS]\\b', '', text)\n",
    "    \n",
    "    # Step 10: Collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Make sure the dataframe is properly filtered before processing\n",
    "news_df = news_df[\n",
    "    news_df['Date'].notna() & \n",
    "    news_df['combined_headlines'].notna() & \n",
    "    (news_df['combined_headlines'].str.strip() != '')\n",
    "]\n",
    "\n",
    "# Apply normalization\n",
    "news_df['normalized_text'] = news_df['combined_headlines'].apply(normalize_text)\n",
    "\n",
    "# This one is the headlines divided individually instead of a per-day basis of concatenation.\n",
    "# As stated, this might be useful later on to track the sentimental derivative\n",
    "# of specific entities. This line applies our normalize function to every h in lst,\n",
    "# or specifically every headline in every row and column.\n",
    "news_df['normalized_separate'] = news_df['separate_headlines'].apply(\n",
    "    lambda lst: [normalize_text(h) for h in lst if h]  # Added check for empty headlines\n",
    ")\n",
    "\n",
    "# Display a sample of normalized text\n",
    "print(\"Original headline:\")\n",
    "print(news_df['combined_headlines'].iloc[0][:400])\n",
    "print(\"\\nNormalized headline:\")\n",
    "print(news_df['normalized_text'].iloc[0][:400])\n",
    "print(\"\\nOriginal Separated headline:\")\n",
    "print(news_df['separate_headlines'].iloc[0][:10])\n",
    "print(\"\\nNormalized Separated headline:\")\n",
    "print(news_df['normalized_separate'].iloc[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b2df9-8c86-451c-ba23-3e2d7e4510c8",
   "metadata": {},
   "source": [
    "We are getting there, Now its time for tokenization! \n",
    "# **Tokenization**\n",
    "\n",
    " Tokenization is the process of breaking text into individual words or tokens, I will use NLTK for this as spacy is not supported with python 3.13...\n",
    "\n",
    " We stored the normalized dataset in the variable \"normalized_text\", lets play around with it, and tokenize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8bfad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize spaCy with custom tokenizer settings\n",
    "nlp = spacy.blank(\"en\")\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "# Add special cases to the tokenizer\n",
    "special_cases = [{\"ORTH\": \"covid19\"}, {\"ORTH\": \"covid-19\"}]\n",
    "for case in special_cases:\n",
    "    nlp.tokenizer.add_special_case(case[\"ORTH\"], [case])\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Splits text into individual words using spaCy's tokenizer with error handling\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    try:\n",
    "        tokens = tokenizer(text)\n",
    "        # Return only non-empty tokens (optional: filter stop words)\n",
    "        # Uncomment the stop words filter if needed\n",
    "        # return [token.text for token in tokens if token.text.strip() and token.text.lower() not in STOP_WORDS]\n",
    "        return [token.text for token in tokens if token.text.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e} in text: {text[:50]}...\")\n",
    "        return []\n",
    "\n",
    "# Function for batch processing\n",
    "def batch_tokenize(texts, batch_size=1000):\n",
    "    \"\"\"Process texts in batches for better performance\"\"\"\n",
    "    # For simple tokenization, we can use the tokenizer directly\n",
    "    return [tokenize_text(text) for text in texts]\n",
    "    \n",
    "    # For more complex NLP tasks, use spaCy's pipe:\n",
    "    # results = []\n",
    "    # for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "    #     results.append([token.text for token in doc])\n",
    "    # return results\n",
    "\n",
    "# Apply tokenization to all texts\n",
    "news_df['tokens'] = batch_tokenize(news_df['normalized_text'])\n",
    "\n",
    "# For separate headlines - with error handling\n",
    "def tokenize_headline_list(headline_list):\n",
    "    if not headline_list:\n",
    "        return []\n",
    "    return [tokenize_text(h) for h in headline_list]\n",
    "\n",
    "news_df['separate_tokens'] = news_df['normalized_separate'].apply(tokenize_headline_list)\n",
    "\n",
    "# Display samples of all three token types\n",
    "print(\"1. COMBINED TEXT TOKENS (first 40):\")\n",
    "print(news_df['tokens'].iloc[0][:40])\n",
    "\n",
    "print(\"\\n2. SEPARATE HEADLINE TOKENS (first 5 headlines):\")\n",
    "sample_headlines = news_df['separate_tokens'].iloc[0][:5] # Get first 5 headlines from first row\n",
    "for i, headline_tokens in enumerate(sample_headlines):\n",
    "    print(f\" Headline {i+1}: {headline_tokens}\")\n",
    "\n",
    "# Token statistics with error handling\n",
    "news_df['token_count'] = news_df['tokens'].apply(len)\n",
    "news_df['avg_token_length'] = news_df['tokens'].apply(\n",
    "    lambda x: np.mean([len(token) for token in x]) if x else 0\n",
    ")\n",
    "\n",
    "# separate_headlines total token count per day\n",
    "news_df['separate_token_count'] = news_df['separate_tokens'].apply(\n",
    "    lambda lst: sum(len(tokens) for tokens in lst) if lst else 0\n",
    ")\n",
    "\n",
    "# compute average word length with error handling\n",
    "news_df['separate_avg_token_length'] = news_df['separate_tokens'].apply(\n",
    "    lambda lst: np.mean([len(token) for tokens in lst for token in tokens]) if lst and any(tokens for tokens in lst) else 0\n",
    ")\n",
    "\n",
    "# each headline is treated like a sentence here\n",
    "news_df['separate_sentence_count'] = news_df['normalized_separate'].apply(len)\n",
    "\n",
    "# graph month column\n",
    "news_df['Month'] = pd.to_datetime(news_df['Date']).dt.to_period('M').astype(str)\n",
    "\n",
    "# Display statistics about all three token types\n",
    "print(\"\\nSTATISTICS SUMMARY:\")\n",
    "print(f\"Combined text tokens: {news_df['token_count'].sum():,} total tokens\")\n",
    "print(f\"Separate headlines: {news_df['separate_sentence_count'].sum():,} total headlines\")\n",
    "print(f\"Separate headline tokens: {news_df['separate_token_count'].sum():,} total tokens\")\n",
    "\n",
    "# Visualization code\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Boxplot of token counts by month\n",
    "sns.boxplot(x='Month', y='token_count', data=news_df, ax=axes[0])\n",
    "axes[0].set_title('Distribution of Token Counts by Month')\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('Token Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Prepare data for grouped bar chart\n",
    "news_df['separate_avg_tokens'] = news_df['separate_token_count'] / news_df['separate_sentence_count'].clip(lower=1)\n",
    "\n",
    "monthly_stats = news_df.groupby('Month').agg({\n",
    "    'token_count': 'mean',\n",
    "    'separate_token_count': 'mean',\n",
    "    'separate_avg_tokens': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "# Barplot of average token count by month\n",
    "sns.barplot(x='Month', y='token_count', data=monthly_stats, ax=axes[1], color='blue', alpha=0.7)\n",
    "axes[1].set_title('Average Tokens per Day')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Token Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd47086-c61d-4733-b5e8-df3badfb2dfd",
   "metadata": {},
   "source": [
    "# **Stop Words Removal**\n",
    "Next, we should tidy up a bit more until we are left with only the meaningful segment of our data in hand. This meaningful segment includes people, places, and everything else that we call entities, while it doesn't include things like \"of, on, and, in, the,\" AND words that dont have any importance in tracking, we care about who or what is being talked about, not the verb they are doing. For example, in the sentence \"Donald Trump stated that China should revert all tariffs back or...\" We are interested in Donald Trump, China, tariffs, but we are not interested in \"that, stated, all, or\" . Revert and should can be used for sentiment analysis, so they should stay. We are also allowing some words, contrary to the stop word list so that stop words don't skew the sentiment analysis data. We also clean up apostrophes so we don't get multiple tokens for words like doesn't, or Trump's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f3b1c-19d0-4469-b6d4-82d792304567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load spaCy English model - only if needed for other purposes\n",
    "# Use blank model if only stopwords are needed to improve performance\n",
    "nlp = spacy.blank(\"en\")  # Changed from spacy.load to save memory/time\n",
    "\n",
    "# Base stop words from spaCy\n",
    "stop_words = STOP_WORDS.copy()\n",
    "\n",
    "# Add your custom stop words\n",
    "custom_stops = {\n",
    "    'says', 'said', 'reuters', 'ap', 'afp', 'report', 'reports', \n",
    "    'or', 'stated', 'for', 'new', 'u', 'amp', 'is', 'in', 'news'\n",
    "}\n",
    "stop_words.update(custom_stops)\n",
    "\n",
    "# Keep important stopwords for sentiment analysis\n",
    "stop_words.difference_update({'not', 'very', 'so', 'should', 'if'})\n",
    "\n",
    "# Function to remove stopwords with error handling\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from a token list with error handling\"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    try:\n",
    "        return [token for token in tokens if token.lower() not in stop_words]\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing stopwords: {e}\")\n",
    "        return tokens\n",
    "\n",
    "# Function to process nested lists (one list per headline)\n",
    "# and DROP any headline that becomes empty after stop-word removal\n",
    "def remove_stopwords_from_separate_headlines(headline_tokens_list):\n",
    "    \"\"\"Process nested token lists and filter empty results\"\"\"\n",
    "    if not headline_tokens_list:\n",
    "        return []\n",
    "        \n",
    "    cleaned_headlines = []\n",
    "    for tokens in headline_tokens_list:\n",
    "        filtered = remove_stopwords(tokens)\n",
    "        if filtered:  # keep only non-empty results\n",
    "            cleaned_headlines.append(filtered)\n",
    "    return cleaned_headlines\n",
    "\n",
    "# Apply the functions - with duplication check\n",
    "if 'tokens_nostop' not in news_df.columns:\n",
    "    news_df['tokens_nostop'] = news_df['tokens'].apply(remove_stopwords)\n",
    "\n",
    "if 'separate_tokens_nostop' not in news_df.columns:\n",
    "    news_df['separate_tokens_nostop'] = news_df['separate_tokens'].apply(\n",
    "        remove_stopwords_from_separate_headlines\n",
    "    )\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Original token count: {sum(news_df['tokens'].apply(len)):,}\")\n",
    "print(f\"Tokens after stopword removal: {sum(news_df['tokens_nostop'].apply(len)):,}\")\n",
    "percent_reduction = (1 - sum(news_df['tokens_nostop'].apply(len))/sum(news_df['tokens'].apply(len))) * 100\n",
    "print(f\"Reduction: {percent_reduction:.2f}%\")\n",
    "\n",
    "# Check for any empty token lists after stopword removal\n",
    "empty_token_rows = news_df[news_df['tokens_nostop'].apply(len) == 0]\n",
    "if not empty_token_rows.empty:\n",
    "    print(f\"\\nWarning: {len(empty_token_rows)} rows have empty token lists after stopword removal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cfb0a1-c25e-414a-9546-baf6db42a8fd",
   "metadata": {},
   "source": [
    "# **Token Unification** \n",
    "If we listed the tokens and how many times they are used, we would see duplicate tokens like covid (2000 uses) and coronavirus (1500 uses). To counter this we create an entity renormalization table of synonyms that will unify some tokens together based on the rules we provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22127f93-0e98-4069-a0d2-569253a5241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym map for token normalization\n",
    "synonym_map = {\n",
    "    # COVID-related terms\n",
    "    'covid': 'covid19',\n",
    "    'covid-19': 'covid19',\n",
    "    'coronavirus': 'covid19',\n",
    "    'covid19': 'covid19',\n",
    "    'corona': 'covid19',  \n",
    "    'corona-virus': 'covid19',\n",
    "    'sars-cov-2': 'covid19',\n",
    "    'sarscov2': 'covid19',\n",
    "\n",
    "    # US related terms\n",
    "    'us': 'usa',\n",
    "    'u.s.': 'usa',\n",
    "    'u.s.a.': 'usa',\n",
    "    'united states': 'usa',\n",
    "    'america': 'usa',\n",
    "    'american': 'usa',\n",
    "    'americans': 'usa',\n",
    "\n",
    "    # UK related terms\n",
    "    'uk': 'united_kingdom',\n",
    "    'u.k.': 'united_kingdom',\n",
    "    'britain': 'united_kingdom',\n",
    "    'british': 'united_kingdom',\n",
    "\n",
    "    # Political terms\n",
    "    'democrat': 'democrats',\n",
    "    'democratic': 'democrats',\n",
    "    'republican': 'republicans',\n",
    "    'gop': 'republicans',\n",
    "\n",
    "    # Politicians\n",
    "    'biden': 'joe_biden',\n",
    "    'joe': 'joe_biden',\n",
    "    'president biden': 'joe_biden',\n",
    "    \n",
    "    'trump': 'donald_trump',\n",
    "    'donald': 'donald_trump',\n",
    "    'president trump': 'donald_trump',\n",
    "    \n",
    "    'putin': 'vladimir_putin',\n",
    "    'vladimir': 'vladimir_putin',\n",
    "    'president putin': 'vladimir_putin',\n",
    "    \n",
    "    # Common variations\n",
    "    'govt': 'government',\n",
    "    'gov': 'government',\n",
    "    'admin': 'administration',\n",
    "    'intl': 'international',\n",
    "    'corp': 'corporation',\n",
    "    'co': 'company',\n",
    "    'cos': 'companies',\n",
    "    'ceo': 'chief_executive_officer',\n",
    "}\n",
    "\n",
    "# Create case-insensitive mapping for better matching\n",
    "case_insensitive_map = {k.lower(): v for k, v in synonym_map.items()}\n",
    "\n",
    "# Enhanced token unification with multi-word phrase handling and error checking\n",
    "def unify_tokens_with_phrases(tokens, mapping):\n",
    "    \"\"\"Normalize tokens using a synonym mapping with multi-word phrase support\"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "        \n",
    "    try:\n",
    "        normalized_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            # Check for multi-word phrases (up to 3 words)\n",
    "            found_match = False\n",
    "            for n in range(min(3, len(tokens) - i), 0, -1):  # Safe range checking\n",
    "                if i + n <= len(tokens):\n",
    "                    phrase = ' '.join(tokens[i:i+n]).lower()\n",
    "                    if phrase in mapping:\n",
    "                        normalized_tokens.append(mapping[phrase])\n",
    "                        i += n\n",
    "                        found_match = True\n",
    "                        break\n",
    "            \n",
    "            # If no phrase match, just normalize the single token\n",
    "            if not found_match:\n",
    "                token_lower = tokens[i].lower()\n",
    "                normalized_tokens.append(mapping.get(token_lower, tokens[i]))\n",
    "                i += 1\n",
    "        \n",
    "        return normalized_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error in token unification: {e} for tokens: {tokens[:5]}...\")\n",
    "        return tokens\n",
    "\n",
    "# Function to safely process nested headline tokens\n",
    "def unify_headline_tokens(headline_list, mapping):\n",
    "    \"\"\"Process a list of headline token lists\"\"\"\n",
    "    if not headline_list:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        return [unify_tokens_with_phrases(tokens, mapping) for tokens in headline_list]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing headline tokens: {e}\")\n",
    "        return headline_list\n",
    "\n",
    "# Apply the enhanced normalization with phrase detection\n",
    "if 'tokens_nostop' in news_df.columns:\n",
    "    news_df['tokens_normalized'] = news_df['tokens_nostop'].apply(\n",
    "        lambda tokens: unify_tokens_with_phrases(tokens, case_insensitive_map)\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: 'tokens_nostop' column not found. Please run stopword removal first.\")\n",
    "\n",
    "# For separate headline tokens\n",
    "if 'separate_tokens_nostop' in news_df.columns:\n",
    "    news_df['separate_tokens_normalized'] = news_df['separate_tokens_nostop'].apply(\n",
    "        lambda headline_list: unify_headline_tokens(headline_list, case_insensitive_map)\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: 'separate_tokens_nostop' column not found. Please run stopword removal first.\")\n",
    "\n",
    "# Calculate token frequencies after normalization to check if duplicates were consolidated\n",
    "from collections import Counter\n",
    "\n",
    "# Only continue if required columns exist\n",
    "if 'tokens_normalized' in news_df.columns:\n",
    "    # Gather statistics\n",
    "    all_normalized_tokens = [token for tokens in news_df['tokens_normalized'].tolist() for token in tokens]\n",
    "    normalized_token_freq = Counter(all_normalized_tokens)\n",
    "    print(\"\\n--- TOP 20 NORMALIZED TOKENS ---\")\n",
    "    print(normalized_token_freq.most_common(20))\n",
    "\n",
    "    # Compare frequencies before and after normalization\n",
    "    if 'tokens_nostop' in news_df.columns:\n",
    "        all_tokens_nostop = [token.lower() for tokens in news_df['tokens_nostop'].tolist() for token in tokens]\n",
    "        token_freq_before = Counter(all_tokens_nostop)\n",
    "        \n",
    "        # Calculate the number of unique tokens before and after normalization\n",
    "        unique_before = len(token_freq_before)\n",
    "        unique_after = len(normalized_token_freq)\n",
    "        \n",
    "        print(f\"\\nUnique tokens before normalization: {unique_before}\")\n",
    "        print(f\"Unique tokens after normalization: {unique_after}\")\n",
    "        print(f\"Reduction in unique tokens: {unique_before - unique_after} ({(1 - unique_after/unique_before)*100:.2f}%)\")\n",
    "        \n",
    "        # Show examples of normalized terms\n",
    "        print(\"\\nSample normalizations:\")\n",
    "        for original, normalized in [('covid', 'covid19'), ('us', 'usa'), ('trump', 'donald_trump')]:\n",
    "            orig_count = token_freq_before.get(original, 0)\n",
    "            norm_count = normalized_token_freq.get(case_insensitive_map.get(original, original), 0)\n",
    "            if orig_count > 0 or norm_count > 0:\n",
    "                print(f\"  '{original}' → '{case_insensitive_map.get(original)}': {orig_count} → {norm_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f3e70-53f0-4258-9fa7-43ff02fdb17e",
   "metadata": {},
   "source": [
    "Inspect the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c23111-881e-4320-b9a5-58b83bba8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt # Import for plotting\n",
    "import pandas as pd # Assuming news_df is a pandas DataFrame\n",
    "\n",
    "# --- TOKEN COUNTS & FREQUENCIES ---\n",
    "print(\"\\n--- TOKEN ANALYSIS ---\")\n",
    "\n",
    "# Calculate counts from the combined list of tokens\n",
    "# Assuming 'tokens' represents the concatenated tokens for a row\n",
    "token_count_before = sum(news_df['tokens'].apply(len))\n",
    "token_count_after = sum(news_df['tokens_nostop'].apply(len))\n",
    "\n",
    "print(\"Original total token count:\", token_count_before)\n",
    "print(\"Total token count after stopword removal:\", token_count_after)\n",
    "print(\"Reduction: {:.2f}%\".format((1 - token_count_after / token_count_before) * 100))\n",
    "\n",
    "# Display sample for tokens (from the combined list)\n",
    "print(\"\\nSample before stopword removal (first 20 tokens in the first row):\")\n",
    "# Handle potential empty lists gracefully\n",
    "print(news_df['tokens'].iloc[0][:20] if news_df['tokens'].iloc[0] else \"[]\")\n",
    "print(\"\\nSample after stopword removal (first 20 tokens in the first row):\")\n",
    "# Handle potential empty lists gracefully\n",
    "print(news_df['tokens_nostop'].iloc[0][:20] if news_df['tokens_nostop'].iloc[0] else \"[]\")\n",
    "\n",
    "\n",
    "# Flatten the list of lists of tokens after stopword removal across the entire DataFrame\n",
    "# This collects *all* non-stopword tokens into a single list for frequency counting\n",
    "all_tokens_nostop = [token for tokens_list in news_df['tokens_nostop'].tolist() for token in tokens_list]\n",
    "\n",
    "# Calculate frequency distribution\n",
    "token_freq = Counter(all_tokens_nostop)\n",
    "\n",
    "# Print Top N tokens\n",
    "N = 20\n",
    "print(f\"\\n--- TOP {N} TOKENS WITHOUT STOPWORDS ---\")\n",
    "top_tokens = token_freq.most_common(N)\n",
    "print(top_tokens)\n",
    "\n",
    "# --- VISUALIZATION ---\n",
    "print(f\"\\n--- PLOTTING TOP {N} TOKENS ---\")\n",
    "\n",
    "if top_tokens:\n",
    "    # Prepare data for plotting\n",
    "    tokens, counts = zip(*top_tokens)\n",
    "\n",
    "    plt.figure(figsize=(12, 7)) # Adjust figure size for better readability\n",
    "    plt.bar(tokens, counts, color='teal') # Using a different color\n",
    "    plt.xlabel('Tokens', fontsize=12) # Increase font size for labels\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title(f'Top {N} Token Frequencies (Without Stopwords)', fontsize=14) # Increase font size for title\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10) # Rotate and adjust alignment of labels\n",
    "    plt.yticks(fontsize=10) # Increase font size for y-axis ticks\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7) # Add a grid for better readability\n",
    "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No tokens found to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07adbe1a",
   "metadata": {},
   "source": [
    "# **Part of Speech Tagging** \n",
    " Now that we have clean, unified tokens, we can perform Part-of-Speech (POS) tagging. \n",
    "\n",
    " POS tagging assigns a grammatical category (like noun, verb, adjective) to each token.\n",
    " \n",
    " This is crucial for understanding sentence structure and meaning.\n",
    "\n",
    " We will be POS tagging and lemmatizing in the same cell, reason explained below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffe835-877f-4582-b075-0607e8132f04",
   "metadata": {},
   "source": [
    "# **Lemmatization**\n",
    "\n",
    "Lemmatization reduces words to their base form (lemma), and we will do this to keep track of trends and sentiment in a more robust way. You might be questioning why we are taking the input of normalized tokens instead of just using the POS tagged column of our dataframe that we will be creating, this is because POS tagged column is made out of a list of list of tuples, while lemmatization requires the input of lists that contain single token strings. The workaround is using POS tagging and lemmatization in a single passtrough of our normalized tokens list, and creating 4 new columns in total, of new POS tagged combined and separate, and lemmatized combined and separate. \n",
    "\n",
    "Lemmatization uses POS tagging as context to strip down them, so POS tagging data is crucial for accurate lemmatization, and for the other reasons explained above, they are in a single cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# LEMMATIZATION WITH POS TAGS FOR HEADLINES\n",
    "# ================================================================\n",
    "import spacy\n",
    "from itertools import compress\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Load spaCy model with lemmatizer enabled, but still disable parser and NER for speed\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\", \"senter\"])\n",
    "\n",
    "# Define a function to safely join tokens\n",
    "def safe_join(tokens):\n",
    "    \"\"\"Safely join tokens into a string with error handling\"\"\"\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return \" \".join(str(tok) for tok in tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error joining tokens: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to safely process a batch with retry for errors\n",
    "def process_batch_with_retry(texts, batch_size=512, n_process=None):\n",
    "    \"\"\"Process a batch of texts with spaCy and retry on failures\"\"\"\n",
    "    results = []\n",
    "    errors = []\n",
    "    \n",
    "    # Use the recommended way to determine processes\n",
    "    if n_process is None:\n",
    "        # Let spaCy decide based on available cores\n",
    "        n_process = -1\n",
    "    \n",
    "    # Process texts with tqdm for progress tracking\n",
    "    with tqdm(total=len(texts), desc=\"Processing texts\") as pbar:\n",
    "        # Safely process with retries on errors\n",
    "        try:\n",
    "            # Process texts in batches\n",
    "            for i, doc in enumerate(nlp.pipe(texts, batch_size=batch_size, n_process=n_process)):\n",
    "                try:\n",
    "                    # Extract text, POS tag, and lemma for each token\n",
    "                    results.append([(tok.text, tok.pos_, tok.lemma_) for tok in doc])\n",
    "                    pbar.update(1)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing text at index {i}: {e}\")\n",
    "                    errors.append(i)\n",
    "                    results.append([])  # Add empty result to maintain index alignment\n",
    "                    pbar.update(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Batch processing error: {e}\")\n",
    "            # Fall back to sequential processing for remaining texts\n",
    "            if errors:\n",
    "                print(f\"Retrying {len(errors)} failed texts sequentially...\")\n",
    "                for i in errors:\n",
    "                    try:\n",
    "                        if i < len(texts):\n",
    "                            doc = nlp(texts[i])\n",
    "                            results[i] = [(tok.text, tok.pos_, tok.lemma_) for tok in doc]\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Sequential retry failed for text at index {i}: {e2}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1)  COMBINED-HEADLINE LEMMATIZATION\n",
    "# ------------------------------------------------\n",
    "print(\"Lemmatizing and POS tagging – combined headlines...\")\n",
    "\n",
    "# Check if required column exists\n",
    "if 'tokens_normalized' not in news_df.columns:\n",
    "    print(\"Warning: 'tokens_normalized' column not found. Please run token normalization first.\")\n",
    "else:\n",
    "    # Build list of texts and remember their DataFrame indices\n",
    "    texts_combined = [safe_join(toks) for toks in news_df['tokens_normalized']]\n",
    "    non_empty_mask = [bool(txt.strip()) for txt in texts_combined]\n",
    "    texts_non_empty = list(compress(texts_combined, non_empty_mask))\n",
    "    idx_mapping = [i for i, keep in enumerate(non_empty_mask) if keep]\n",
    "    \n",
    "    # Show stats\n",
    "    print(f\"Processing {len(texts_non_empty):,} non-empty texts out of {len(texts_combined):,} total\")\n",
    "    \n",
    "    # Handle edge case of no texts\n",
    "    if not texts_non_empty:\n",
    "        warnings.warn(\"No non-empty texts found for lemmatization!\")\n",
    "        lemma_pos_combined = [[] for _ in range(len(news_df))]\n",
    "    else:\n",
    "        # Prepare result container (same length as DataFrame)\n",
    "        lemma_pos_combined = [[] for _ in range(len(news_df))]\n",
    "        \n",
    "        # Determine optimal batch size based on text length\n",
    "        avg_len = sum(len(t) for t in texts_non_empty) / max(1, len(texts_non_empty))\n",
    "        batch_size = max(32, min(512, int(10000 / max(1, avg_len))))\n",
    "        print(f\"Using batch size of {batch_size} based on average text length of {avg_len:.1f} chars\")\n",
    "        \n",
    "        # Determine optimal number of processes\n",
    "        import os\n",
    "        suggested_processes = min(4, os.cpu_count() or 1)\n",
    "        print(f\"Using {suggested_processes} processes for parallel processing\")\n",
    "        \n",
    "        # Run spaCy in batches / multi-process\n",
    "        lemma_results = process_batch_with_retry(\n",
    "            texts_non_empty, \n",
    "            batch_size=batch_size, \n",
    "            n_process=suggested_processes\n",
    "        )\n",
    "        \n",
    "        # Map results back to DataFrame positions\n",
    "        for i, df_idx in enumerate(idx_mapping):\n",
    "            if i < len(lemma_results):\n",
    "                lemma_pos_combined[df_idx] = lemma_results[i]\n",
    "    \n",
    "    # Add to DataFrame\n",
    "    news_df['lemma_pos_combined'] = lemma_pos_combined\n",
    "    \n",
    "    # Create a column with just the lemmatized tokens for convenience\n",
    "    news_df['lemmas_combined'] = [\n",
    "        [item[2] for item in token_list] if token_list else [] \n",
    "        for token_list in news_df['lemma_pos_combined']\n",
    "    ]\n",
    "    \n",
    "    # Quick stats\n",
    "    total_lemmas = sum(len(lemmas) for lemmas in news_df['lemmas_combined'])\n",
    "    print(f\"Total lemmas generated: {total_lemmas:,}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2)  SEPARATE-HEADLINE LEMMATIZATION\n",
    "# ------------------------------------------------\n",
    "print(\"\\nLemmatizing and POS tagging – separate headlines...\")\n",
    "\n",
    "# Check if required column exists\n",
    "if 'separate_tokens_normalized' not in news_df.columns:\n",
    "    print(\"Warning: 'separate_tokens_normalized' column not found. Please run token normalization first.\")\n",
    "else:\n",
    "    texts_sep = []   # flattened texts to send to spaCy\n",
    "    row_map = []     # DataFrame row index for each text\n",
    "    sub_map = []     # sub-list position inside that row\n",
    "    \n",
    "    # Handle potential iteritems vs items method difference (pandas version compatibility)\n",
    "    iter_method = getattr(news_df['separate_tokens_normalized'], 'items', None)\n",
    "    if iter_method is None:\n",
    "        iter_method = news_df['separate_tokens_normalized'].iteritems\n",
    "    \n",
    "    # Collect texts with their mapping information\n",
    "    for row_idx, headline_lists in iter_method():\n",
    "        if not headline_lists:\n",
    "            continue\n",
    "            \n",
    "        for sub_idx, toks in enumerate(headline_lists):\n",
    "            if toks:  # already filtered empties\n",
    "                text = safe_join(toks)\n",
    "                if text.strip():  # Make sure we have actual content\n",
    "                    texts_sep.append(text)\n",
    "                    row_map.append(row_idx)\n",
    "                    sub_map.append(sub_idx)\n",
    "    \n",
    "    # Show stats\n",
    "    print(f\"Processing {len(texts_sep):,} separate headlines\")\n",
    "    \n",
    "    # Handle edge case of no texts\n",
    "    if not texts_sep:\n",
    "        warnings.warn(\"No non-empty separate headlines found for lemmatization!\")\n",
    "        news_df['lemma_pos_separate'] = [[] for _ in range(len(news_df))]\n",
    "        news_df['lemmas_separate'] = [[] for _ in range(len(news_df))]\n",
    "    else:\n",
    "        # Determine optimal batch size based on text length\n",
    "        avg_len = sum(len(t) for t in texts_sep) / max(1, len(texts_sep))\n",
    "        batch_size = max(32, min(512, int(10000 / max(1, avg_len))))\n",
    "        print(f\"Using batch size of {batch_size} based on average text length of {avg_len:.1f} chars\")\n",
    "        \n",
    "        # Determine optimal number of processes\n",
    "        import os\n",
    "        suggested_processes = min(4, os.cpu_count() or 1)\n",
    "        print(f\"Using {suggested_processes} processes for parallel processing\")\n",
    "        \n",
    "        # Lemmatize and POS-tag all in one go with progress display\n",
    "        docs_lemma = process_batch_with_retry(\n",
    "            texts_sep, \n",
    "            batch_size=batch_size, \n",
    "            n_process=suggested_processes\n",
    "        )\n",
    "        \n",
    "        # Re-assemble back into the original nested structure\n",
    "        lemma_dict = defaultdict(list)   # row_idx -> list of lists\n",
    "        \n",
    "        for i, (row_idx, sub_idx) in enumerate(zip(row_map, sub_map)):\n",
    "            if i >= len(docs_lemma):\n",
    "                continue\n",
    "                \n",
    "            lemma_pos = docs_lemma[i]\n",
    "            \n",
    "            # ensure outer list long enough\n",
    "            while len(lemma_dict[row_idx]) <= sub_idx:\n",
    "                lemma_dict[row_idx].append([])\n",
    "            \n",
    "            lemma_dict[row_idx][sub_idx] = lemma_pos\n",
    "        \n",
    "        # Add to DataFrame\n",
    "        news_df['lemma_pos_separate'] = news_df.index.map(lambda i: lemma_dict.get(i, []))\n",
    "        \n",
    "        # Create a nested list with just the lemmas for convenience\n",
    "        news_df['lemmas_separate'] = news_df['lemma_pos_separate'].apply(\n",
    "            lambda headline_lists: [\n",
    "                [item[2] for item in tokens] if tokens else []\n",
    "                for tokens in headline_lists\n",
    "            ] if headline_lists else []\n",
    "        )\n",
    "        \n",
    "        # Quick stats\n",
    "        total_headlines = sum(len(lemmas) for lemmas in news_df['lemmas_separate'])\n",
    "        total_lemmas = sum(len(headline) for headlines in news_df['lemmas_separate'] for headline in headlines)\n",
    "        print(f\"Total headlines processed: {total_headlines:,}\")\n",
    "        print(f\"Total lemmas generated: {total_lemmas:,}\")\n",
    "\n",
    "# Sample display\n",
    "print(\"\\nSample lemmatization results:\")\n",
    "if 'lemma_pos_combined' in news_df.columns and len(news_df) > 0:\n",
    "    sample_lemmas = news_df['lemma_pos_combined'].iloc[0][:5]\n",
    "    print(f\"First 5 combined lemmas with POS: {sample_lemmas}\")\n",
    "    \n",
    "    sample_just_lemmas = news_df['lemmas_combined'].iloc[0][:5]\n",
    "    print(f\"First 5 combined lemmas only: {sample_just_lemmas}\")\n",
    "\n",
    "if 'lemma_pos_separate' in news_df.columns and len(news_df) > 0:\n",
    "    # Get first headline's lemmas from first row\n",
    "    if news_df['lemma_pos_separate'].iloc[0] and news_df['lemma_pos_separate'].iloc[0][0]:\n",
    "        sample_sep_lemmas = news_df['lemma_pos_separate'].iloc[0][0][:5]\n",
    "        print(f\"First 5 lemmas with POS from first separate headline: {sample_sep_lemmas}\")\n",
    "        \n",
    "        sample_sep_just_lemmas = news_df['lemmas_separate'].iloc[0][0][:5] if news_df['lemmas_separate'].iloc[0] else []\n",
    "        print(f\"First 5 lemmas only from first separate headline: {sample_sep_just_lemmas}\")\n",
    "\n",
    "# Display information about the columns we've added\n",
    "print(\"\\nNew columns added to DataFrame:\")\n",
    "print(\"  - lemma_pos_combined: List of (token, POS, lemma) tuples for combined headlines\")\n",
    "print(\"  - lemmas_combined: List of just lemmas for combined headlines\")\n",
    "print(\"  - lemma_pos_separate: Nested lists of (token, POS, lemma) tuples for separate headlines\")\n",
    "print(\"  - lemmas_separate: Nested lists of just lemmas for separate headlines\")\n",
    "\n",
    "# Display a simple visualization of lemmatization effects\n",
    "if 'tokens_normalized' in news_df.columns and 'lemmas_combined' in news_df.columns and len(news_df) > 0:\n",
    "    print(\"\\nLemmatization example (first row):\")\n",
    "    tokens = news_df['tokens_normalized'].iloc[0][:10]  # First 10 tokens\n",
    "    lemmas = news_df['lemmas_combined'].iloc[0][:10]    # First 10 lemmas\n",
    "    \n",
    "    if tokens and lemmas:\n",
    "        print(\"Original tokens vs. Lemmas:\")\n",
    "        for i, (token, lemma) in enumerate(zip(tokens[:10], lemmas[:10])):\n",
    "            if token != lemma:\n",
    "                print(f\"  {token} → {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a905983",
   "metadata": {},
   "source": [
    "# Part Of Speech Data Visualization\n",
    "\n",
    "Lets see what we can learn from all the data we processed so far! We have a lot of data to process. We can start by graphing the POS tagging data, and inspect their various aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# POS TAGGING VISUALIZATION (FIXED VERSION)\n",
    "# ================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "print(\"Generating fixed visualizations for POS tagging data...\")\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1) Distribution of POS Tags (FIXED)\n",
    "# ------------------------------------------------\n",
    "print(\"Analyzing POS tag distributions...\")\n",
    "\n",
    "# Fixed function to count POS tags with better handling of nested structures\n",
    "def count_tags(tags_column):\n",
    "    tag_counts = Counter()\n",
    "    \n",
    "    # Skip empty DataFrames\n",
    "    if len(tags_column) == 0:\n",
    "        return tag_counts\n",
    "        \n",
    "    # Try to get a non-empty sample to determine the data structure\n",
    "    sample = None\n",
    "    for item in tags_column:\n",
    "        if item:  # Find first non-empty item\n",
    "            sample = item\n",
    "            break\n",
    "    \n",
    "    if sample is None:\n",
    "        return tag_counts\n",
    "    \n",
    "    # Handle different formats based on nesting level and tuple structure\n",
    "    if isinstance(sample, list):\n",
    "        # Check if it's the separate headlines format (list of lists of tuples)\n",
    "        if sample and isinstance(sample[0], list):\n",
    "            for headlines in tags_column:\n",
    "                if not headlines:\n",
    "                    continue\n",
    "                for headline in headlines:\n",
    "                    if not headline:\n",
    "                        continue\n",
    "                    # Check tuple format - (text, pos, lemma) or (text, pos)\n",
    "                    if headline and isinstance(headline[0], tuple):\n",
    "                        if len(headline[0]) == 3:  # New format (text, pos, lemma)\n",
    "                            for _, pos, _ in headline:\n",
    "                                tag_counts[pos] += 1\n",
    "                        else:  # Old format (text, pos)\n",
    "                            for _, pos in headline:\n",
    "                                tag_counts[pos] += 1\n",
    "        # It's the combined headlines format (list of tuples)\n",
    "        elif sample and isinstance(sample[0], tuple):\n",
    "            for tags in tags_column:\n",
    "                if not tags:\n",
    "                    continue\n",
    "                # Check tuple format\n",
    "                if len(tags[0]) == 3:  # New format (text, pos, lemma)\n",
    "                    for _, pos, _ in tags:\n",
    "                        tag_counts[pos] += 1\n",
    "                else:  # Old format (text, pos)\n",
    "                    for _, pos in tags:\n",
    "                        tag_counts[pos] += 1\n",
    "    \n",
    "    return tag_counts\n",
    "\n",
    "# Determine which columns to use for visualization\n",
    "pos_columns = []\n",
    "if 'lemma_pos_combined' in news_df.columns:\n",
    "    pos_columns.append(('lemma_pos_combined', 'Combined Headlines'))\n",
    "elif 'pos_tags_combined' in news_df.columns:\n",
    "    pos_columns.append(('pos_tags_combined', 'Combined Headlines'))\n",
    "\n",
    "if 'lemma_pos_separate' in news_df.columns:\n",
    "    pos_columns.append(('lemma_pos_separate', 'Separate Headlines'))\n",
    "elif 'pos_tags_separate' in news_df.columns:\n",
    "    pos_columns.append(('pos_tags_separate', 'Separate Headlines'))\n",
    "\n",
    "# Debug - print sample row data to verify structure\n",
    "print(\"Debugging data structure:\")\n",
    "for col_name, title in pos_columns:\n",
    "    print(f\"\\nColumn: {col_name}\")\n",
    "    # Find first non-empty row\n",
    "    for i, row in enumerate(news_df[col_name]):\n",
    "        if row:\n",
    "            print(f\"Sample row {i}: {type(row)}\")\n",
    "            if isinstance(row, list):\n",
    "                if row and isinstance(row[0], list):\n",
    "                    print(f\"  First nested list: {type(row[0])}\")\n",
    "                    if row[0]:\n",
    "                        print(f\"    First element in nested list: {type(row[0][0])}\")\n",
    "                        print(f\"    First few items: {row[0][:3]}\")\n",
    "                elif row and isinstance(row[0], tuple):\n",
    "                    print(f\"  First tuple: {type(row[0])}\")\n",
    "                    print(f\"  Sample tuples: {row[:3]}\")\n",
    "            break\n",
    "\n",
    "# Create separate figures based on available columns\n",
    "num_plots = len(pos_columns)\n",
    "if num_plots > 0:\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(10*num_plots, 8))\n",
    "    # Handle single plot case\n",
    "    if num_plots == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (col_name, title_suffix) in enumerate(pos_columns):\n",
    "        # Count POS tags with the fixed function\n",
    "        pos_counts = count_tags(news_df[col_name])\n",
    "        \n",
    "        # Debug to verify tag counting is working\n",
    "        print(f\"\\nPOS counts for {col_name}:\")\n",
    "        print(f\"Total unique POS tags found: {len(pos_counts)}\")\n",
    "        if pos_counts:\n",
    "            top_5 = sorted(pos_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            print(f\"Top 5 POS tags: {top_5}\")\n",
    "        else:\n",
    "            print(\"No POS tags found!\")\n",
    "        \n",
    "        if not pos_counts:\n",
    "            axes[i].text(0.5, 0.5, f\"No POS tags found in {title_suffix}\",\n",
    "                         ha='center', va='center', fontsize=12)\n",
    "            axes[i].set_title(f'POS Tags Distribution - {title_suffix}')\n",
    "            axes[i].axis('off')\n",
    "            continue\n",
    "        \n",
    "        pos_df = pd.DataFrame({\n",
    "            'POS': list(pos_counts.keys()),\n",
    "            'Count': list(pos_counts.values())\n",
    "        }).sort_values('Count', ascending=False)\n",
    "        \n",
    "        # Calculate percentage\n",
    "        total = pos_df['Count'].sum()\n",
    "        pos_df['Percentage'] = pos_df['Count'] / total * 100\n",
    "        \n",
    "        # Plot top N tags\n",
    "        top_n = min(15, len(pos_df))\n",
    "        sns.barplot(x='POS', y='Percentage', data=pos_df.head(top_n), ax=axes[i])\n",
    "        axes[i].set_title(f'Top {top_n} POS Tags Distribution - {title_suffix}')\n",
    "        axes[i].set_ylabel('Percentage (%)')\n",
    "        axes[i].set_xlabel('Part of Speech')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for j, p in enumerate(axes[i].patches):\n",
    "            height = p.get_height()\n",
    "            axes[i].text(p.get_x() + p.get_width()/2., height + 0.3,\n",
    "                    f'{height:.1f}%', ha=\"center\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pos_tag_distribution_fixed.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No POS tagging columns found in the DataFrame.\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2) Word Clouds by POS Tag\n",
    "# ------------------------------------------------\n",
    "print(\"Generating word clouds for common POS tags...\")\n",
    "\n",
    "# Function to extract words by POS tag with support for new format\n",
    "def get_words_by_pos(tags_column, target_pos):\n",
    "    words = []\n",
    "    \n",
    "    # Skip empty DataFrames\n",
    "    if len(tags_column) == 0:\n",
    "        return words\n",
    "        \n",
    "    # Check the format of the data\n",
    "    sample = next((x for x in tags_column if x), None)\n",
    "    \n",
    "    if sample is None:\n",
    "        return words\n",
    "        \n",
    "    # Handle different formats\n",
    "    if isinstance(sample, list):\n",
    "        if not sample:\n",
    "            return words\n",
    "            \n",
    "        if isinstance(sample[0], tuple):\n",
    "            # Check if it's the new format with (text, pos, lemma) or old format with (text, pos)\n",
    "            if len(sample[0]) == 3:  # New format with lemma\n",
    "                for tags in tags_column:\n",
    "                    words.extend([word.lower() for word, pos, _ in tags if pos == target_pos])\n",
    "            else:  # Old format\n",
    "                for tags in tags_column:\n",
    "                    words.extend([word.lower() for word, pos in tags if pos == target_pos])\n",
    "    else:\n",
    "        # Handle nested lists (separate headlines)\n",
    "        for headlines in tags_column:\n",
    "            if not headlines:\n",
    "                continue\n",
    "                \n",
    "            for headline in headlines:\n",
    "                if not headline:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if it's the new format\n",
    "                if len(headline[0]) == 3:  # New format with lemma\n",
    "                    words.extend([word.lower() for word, pos, _ in headline if pos == target_pos])\n",
    "                else:  # Old format\n",
    "                    words.extend([word.lower() for word, pos in headline if pos == target_pos])\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Select important POS categories to visualize\n",
    "important_pos = ['NOUN', 'VERB', 'ADJ', 'PROPN']\n",
    "\n",
    "# Choose which column to use for word clouds (prioritize lemma_pos_combined)\n",
    "wordcloud_col = None\n",
    "if 'lemma_pos_combined' in news_df.columns:\n",
    "    wordcloud_col = 'lemma_pos_combined'\n",
    "elif 'pos_tags_combined' in news_df.columns:\n",
    "    wordcloud_col = 'pos_tags_combined'\n",
    "elif 'lemma_pos_separate' in news_df.columns:\n",
    "    wordcloud_col = 'lemma_pos_separate'\n",
    "elif 'pos_tags_separate' in news_df.columns:\n",
    "    wordcloud_col = 'pos_tags_separate'\n",
    "\n",
    "if wordcloud_col:\n",
    "    # Create word clouds for important POS tags\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Use the selected column for word clouds\n",
    "    for i, pos in enumerate(important_pos):\n",
    "        words = get_words_by_pos(news_df[wordcloud_col], pos)\n",
    "        word_counts = Counter(words)\n",
    "        \n",
    "        # Skip if no words found\n",
    "        if not word_counts:\n",
    "            axes[i].text(0.5, 0.5, f\"No {pos} tags found\", \n",
    "                         horizontalalignment='center', fontsize=18)\n",
    "            axes[i].axis('off')\n",
    "            continue\n",
    "        \n",
    "        # Generate word cloud\n",
    "        wc = WordCloud(width=800, height=400, \n",
    "                       background_color='white', \n",
    "                       max_words=100,\n",
    "                       colormap='viridis',\n",
    "                       contour_width=1, contour_color='steelblue')\n",
    "        \n",
    "        wc.generate_from_frequencies(word_counts)\n",
    "        \n",
    "        axes[i].imshow(wc, interpolation='bilinear')\n",
    "        axes[i].set_title(f'Most Common {pos} Words', fontsize=18)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0)  # Try smaller values like 0.1 or 0.05 if needed\n",
    "    plt.savefig('pos_word_clouds.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No suitable columns found for word cloud visualization.\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) Headline POS Tag Sequence Patterns\n",
    "# ------------------------------------------------\n",
    "print(\"Analyzing common POS tag sequences...\")\n",
    "\n",
    "# Function to extract common POS sequences with support for new format\n",
    "def get_pos_sequences(tags_column, seq_length=3):\n",
    "    sequences = []\n",
    "    \n",
    "    # Skip empty DataFrames\n",
    "    if len(tags_column) == 0:\n",
    "        return sequences\n",
    "        \n",
    "    # Check the format of the data\n",
    "    sample = next((x for x in tags_column if x), None)\n",
    "    \n",
    "    if sample is None:\n",
    "        return sequences\n",
    "        \n",
    "    # Handle different formats\n",
    "    if isinstance(sample, list):\n",
    "        if not sample:\n",
    "            return sequences\n",
    "            \n",
    "        if isinstance(sample[0], tuple):\n",
    "            # Check if it's the new format with (text, pos, lemma) or old format with (text, pos)\n",
    "            if len(sample[0]) == 3:  # New format with lemma\n",
    "                for tags in tags_column:\n",
    "                    if len(tags) >= seq_length:\n",
    "                        pos_tags = [pos for _, pos, _ in tags]\n",
    "                        for i in range(len(pos_tags) - seq_length + 1):\n",
    "                            sequences.append(tuple(pos_tags[i:i+seq_length]))\n",
    "            else:  # Old format\n",
    "                for tags in tags_column:\n",
    "                    if len(tags) >= seq_length:\n",
    "                        pos_tags = [pos for _, pos in tags]\n",
    "                        for i in range(len(pos_tags) - seq_length + 1):\n",
    "                            sequences.append(tuple(pos_tags[i:i+seq_length]))\n",
    "    else:\n",
    "        # Handle nested lists (separate headlines)\n",
    "        for headlines in tags_column:\n",
    "            if not headlines:\n",
    "                continue\n",
    "                \n",
    "            for headline in headlines:\n",
    "                if len(headline) >= seq_length:\n",
    "                    # Check if it's the new format\n",
    "                    if len(headline[0]) == 3:  # New format with lemma\n",
    "                        pos_tags = [pos for _, pos, _ in headline]\n",
    "                        for i in range(len(pos_tags) - seq_length + 1):\n",
    "                            sequences.append(tuple(pos_tags[i:i+seq_length]))\n",
    "                    else:  # Old format\n",
    "                        pos_tags = [pos for _, pos in headline]\n",
    "                        for i in range(len(pos_tags) - seq_length + 1):\n",
    "                            sequences.append(tuple(pos_tags[i:i+seq_length]))\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Choose which column to use for sequence analysis (prioritize lemma_pos_combined)\n",
    "sequence_col = None\n",
    "if 'lemma_pos_combined' in news_df.columns:\n",
    "    sequence_col = 'lemma_pos_combined'\n",
    "elif 'pos_tags_combined' in news_df.columns:\n",
    "    sequence_col = 'pos_tags_combined'\n",
    "elif 'lemma_pos_separate' in news_df.columns:\n",
    "    sequence_col = 'lemma_pos_separate'\n",
    "elif 'pos_tags_separate' in news_df.columns:\n",
    "    sequence_col = 'pos_tags_separate'\n",
    "\n",
    "if sequence_col:\n",
    "    seq_length = 3  # trigrams\n",
    "    sequences = get_pos_sequences(news_df[sequence_col], seq_length)\n",
    "    seq_counts = Counter(sequences)\n",
    "    \n",
    "    # Create DataFrame for top sequences\n",
    "    top_n = 15\n",
    "    seq_df = pd.DataFrame({\n",
    "        'Sequence': [' → '.join(seq) for seq in seq_counts.keys()],\n",
    "        'Count': list(seq_counts.values())\n",
    "    }).sort_values('Count', ascending=False).head(top_n)\n",
    "    \n",
    "    # Calculate percentage\n",
    "    total = sum(seq_counts.values())\n",
    "    seq_df['Percentage'] = seq_df['Count'] / total * 100\n",
    "    \n",
    "    # Plot top sequences\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    bars = sns.barplot(x='Sequence', y='Percentage', data=seq_df)\n",
    "    plt.title(f'Top {top_n} POS Tag Sequences (Length {seq_length})', fontsize=16)\n",
    "    plt.xlabel('POS Tag Sequence', fontsize=14)\n",
    "    plt.ylabel('Percentage (%)', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, p in enumerate(bars.patches):\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width()/2., height + 0.1,\n",
    "                f'{height:.1f}%', ha=\"center\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pos_sequence_patterns.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No suitable columns found for sequence pattern analysis.\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) Headline Length Distribution by POS Count\n",
    "# ------------------------------------------------\n",
    "print(\"Analyzing headline length distributions...\")\n",
    "\n",
    "# Function to get POS tag counts per headline with support for new format\n",
    "def get_pos_counts_per_headline(tags_column):\n",
    "    counts = []\n",
    "    if tags_column.empty:\n",
    "        return counts\n",
    "\n",
    "    sample = tags_column.iloc[0]\n",
    "\n",
    "    if all(isinstance(elem, tuple) for elem in sample):\n",
    "        # Combined format: one headline per row\n",
    "        for tags in tags_column:\n",
    "            counts.append(len(tags))\n",
    "    else:\n",
    "        # Separate format: multiple headlines per row\n",
    "        for headlines in tags_column:\n",
    "            if not headlines:\n",
    "                continue\n",
    "            for headline in headlines:\n",
    "                counts.append(len(headline))\n",
    "    return counts\n",
    "\n",
    "# Choose columns for length distribution // i crossed out combined headlines because they are not relevant here. \n",
    "length_columns = []\n",
    "#if 'lemma_pos_combined' in news_df.columns:\n",
    "#    length_columns.append(('lemma_pos_combined', 'Combined Headlines', 'blue'))\n",
    "#elif 'pos_tags_combined' in news_df.columns:\n",
    "#    length_columns.append(('pos_tags_combined', 'Combined Headlines', 'blue'))\n",
    "\n",
    "if 'lemma_pos_separate' in news_df.columns:\n",
    "    length_columns.append(('lemma_pos_separate', 'Separate Headlines', 'green'))\n",
    "elif 'pos_tags_separate' in news_df.columns:\n",
    "    length_columns.append(('pos_tags_separate', 'Separate Headlines', 'green'))\n",
    "\n",
    "# Plot headline length distribution if at least one column is available\n",
    "if length_columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for col_name, label, color in length_columns:\n",
    "        counts = get_pos_counts_per_headline(news_df[col_name])\n",
    "        sns.histplot(counts, color=color, alpha=0.6, label=label, bins=20)\n",
    "    \n",
    "    plt.title('Distribution of Headline Lengths (Word Count)', fontsize=16)\n",
    "    plt.xlabel('Number of Words', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.xlim(0, 32)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('headline_length_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No suitable columns found for headline length analysis.\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) Comparative Analysis Between Different Headlines\n",
    "# ------------------------------------------------\n",
    "print(\"Analyzing POS tag variations across headlines...\")\n",
    "\n",
    "# Choose which column to use for comparative analysis (prioritize lemma_pos_separate)\n",
    "comparison_col = None\n",
    "if 'lemma_pos_separate' in news_df.columns:\n",
    "    comparison_col = 'lemma_pos_separate'\n",
    "elif 'pos_tags_separate' in news_df.columns:\n",
    "    comparison_col = 'pos_tags_separate'\n",
    "\n",
    "if comparison_col:\n",
    "    # Function to calculate POS tag diversity with support for new format\n",
    "    def calculate_pos_diversity(headlines_tags):\n",
    "        headline_diversities = []\n",
    "        \n",
    "        for headlines in headlines_tags:\n",
    "            if len(headlines) >= 2:  # Need at least 2 headlines to compare\n",
    "                # Calculate Jaccard similarity between headlines\n",
    "                similarities = []\n",
    "                for i in range(len(headlines)):\n",
    "                    for j in range(i+1, len(headlines)):\n",
    "                        # Check if it's the new format\n",
    "                        if len(headlines[i][0]) == 3 and len(headlines[j][0]) == 3:  # New format with lemma\n",
    "                            # Extract POS tags\n",
    "                            pos_i = [pos for _, pos, _ in headlines[i]]\n",
    "                            pos_j = [pos for _, pos, _ in headlines[j]]\n",
    "                        else:  # Old format\n",
    "                            # Extract POS tags\n",
    "                            pos_i = [pos for _, pos in headlines[i]]\n",
    "                            pos_j = [pos for _, pos in headlines[j]]\n",
    "                        \n",
    "                        # Calculate Jaccard similarity (intersection / union)\n",
    "                        set_i = set(pos_i)\n",
    "                        set_j = set(pos_j)\n",
    "                        intersection = len(set_i.intersection(set_j))\n",
    "                        union = len(set_i.union(set_j))\n",
    "                        \n",
    "                        if union > 0:\n",
    "                            similarities.append(intersection / union)\n",
    "                \n",
    "                # Average similarity for this row\n",
    "                if similarities:\n",
    "                    headline_diversities.append(np.mean(similarities))\n",
    "        \n",
    "        return headline_diversities\n",
    "    \n",
    "    # Calculate diversity scores\n",
    "    diversity_scores = calculate_pos_diversity(news_df[comparison_col])\n",
    "    \n",
    "    # Plot diversity distribution if we have enough data\n",
    "    if diversity_scores:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.histplot(diversity_scores, bins=20, kde=True)\n",
    "        plt.title('POS Tag Similarity Between Headlines (Jaccard Index)', fontsize=16)\n",
    "        plt.xlabel('Average Jaccard Similarity (higher = more similar)', fontsize=14)\n",
    "        plt.ylabel('Frequency', fontsize=14)\n",
    "        plt.xlim(0, 1)\n",
    "        plt.axvline(np.mean(diversity_scores), color='red', linestyle='--', \n",
    "                    label=f'Mean: {np.mean(diversity_scores):.2f}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('headline_pos_similarity.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Not enough comparable headlines found for diversity analysis.\")\n",
    "else:\n",
    "    print(\"No suitable columns found for comparative analysis.\")\n",
    "\n",
    "print(\"Visualization complete! All plots have been saved as PNG files.\")\n",
    "\n",
    "print(\"Sample Separate Headlines data:\", news_df['lemma_pos_separate'].dropna().head(3).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a4d01",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "We have some interesting remarks already. We find that:\n",
    " 63% of news follow a similar grammatical trend, \n",
    " Covid is NOT the most talked about topic even though it felt like that living through that era,\n",
    " Russia, China and Ukraine was the 3 countries that news outlets talked the most about, \n",
    " 15.3% of 3 word sequences are made out of nouns,\n",
    " Most of news headlines are 5 to 10 words,\n",
    "\n",
    "\n",
    " TO DO:\n",
    " Most common verbs\n",
    " Most common Nouns\n",
    " Most common phrases of words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c838d-dd5e-485c-88ef-7283670c3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c9ea2-23cf-4a41-981f-66fab9a26f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
